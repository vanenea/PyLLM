# 知识库智能体QA应用设计文档

## 1. 简介

本设计文档详细描述了知识库智能体QA应用的架构与实现，该应用能够加载本地的 TXT 和 DOCX 文档，将其拆分成可管理的文本块，使用嵌入式向量存储进行索引，并通过交互式控制台接口，基于加载的语料库为用户提供问答功能。

### 1.1 目的

* 为开发者提供清晰的蓝图，便于理解、维护并扩展知识库智能体QA系统。
* 详细说明系统架构、组件、数据流以及配置方法。

### 1.2 范围

* 文档加载（TXT、DOCX）
* 文本拆分
* 自定义嵌入接口（阿里云百炼向量DashScope）
* 向量存储（Chroma）
* 检索式问答链（LangChain + DeepSeek LLM）
* 交互式控制台循环

## 2. 系统概览

应用主要由以下逻辑模块组成：

1. **初始化**：加载环境变量与 NLTK 资源。
2. **文档加载器**：从指定目录发现并加载 TXT 与 DOCX 文件。
3. **文本拆分器**：将文档内容拆分为便于嵌入的小块。
4. **阿里云百炼向量库（文本嵌入）**：自定义包装阿里云百炼的向量库 DashScope 兼容的 OpenAI 嵌入。
5. **向量存储**：使用 Chroma 持久化嵌入向量。
6. **QA 链构造器**：基于 LangChain 和 DeepSeek API 构建检索式问答链。
7. **控制台接口**：提供交互式问答循环。

## 3. 依赖项

* Python 3.8 及以上
* langchain-core==0.3.59
* langchain-community==0.3.23
* langchain-openai==0.3.16  
* langchain-text-splitters==0.3.8
* python-dotenv==0.21.0
* sentence-transformers==2.6.0  # 新增本地Embedding支持
* openai==1.77.0
* nltk==3.8.1

所有依赖在 `requirements.txt` 中列出，以确保环境可复现。

## 4. 配置与环境变量(.env)

* **DASHSCOPE\_API\_KEY**：阿里云百炼向量DashScope 嵌入服务的 API 密钥
* **DEEPSEEK\_API\_KEY**：DeepSeek 聊天模型的 API 密钥

使用 `load_dotenv()` 从 `.env` 文件读取上述变量。

## 5. 模块说明

### 5.1 NLTK 初始化
* 检查 `punkt` 分句器是否存在，若缺失则下载至 `./nltk_data`。
* 确保 `averaged_perceptron_tagger` 可用。
```python
import nltk
def init_nltk():
    """初始化NLTK资源"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("正在自动下载NLTK资源...")
        nltk.download('punkt', download_dir='./nltk_data', quiet=True)
        nltk.download('averaged_perceptron_tagger')
        nltk.data.path.append('./nltk_data')
init_nltk()
```

### 5.2 阿里云百炼向量库（文本嵌入）：`DashScopeEmbeddings`

* **构造函数**：接收 `model_name`、`api_key` 和 `base_url`。
* **embed\_documents(texts)**：批量调用兼容 OpenAI 接口进行文本嵌入。
* **embed\_query(text)**：针对单条查询生成嵌入向量。
```python
from openai import OpenAI
class DashScopeEmbeddings:
    """自定义 Embeddings，适配阿里云 DashScope OpenAI 兼容接口"""
    def __init__(self, model_name: str, api_key: str, base_url: str):
        self.model = model_name
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        # 传入 list[str]，返回 list of embedding vectors
        resp = self.client.embeddings.create(
            model=self.model,
            input=texts
        )
        # resp.data 是一个列表，每项有 .embedding 属性
        return [d.embedding for d in resp.data]

    def embed_query(self, text: str) -> list[float]:
        # 针对单条查询
        resp = self.client.embeddings.create(
            model=self.model,
            input=text
        )
        return resp.data[0].embedding
```

### 5.3 文档加载（加载docs目录下的txt和docs文件）

* 使用 `DirectoryLoader` 和 `TextLoader` 加载 `**/*.txt` 文件。
* 使用 `DirectoryLoader` 和 `UnstructuredWordDocumentLoader` 加载 `**/*.docx` 文件。
* 将 `txt_docs` 与 `word_docs` 合并。
```python
from langchain_community.document_loaders import DirectoryLoader
from langchain_community.document_loaders import TextLoader, UnstructuredWordDocumentLoader
# 1. 读取所有 txt
txt_loader = DirectoryLoader("docs/", glob="**/*.txt", loader_cls= TextLoader)
txt_docs = txt_loader.load()

# 2. 读取所有 docx
word_loader = DirectoryLoader(
    "docs/",
    glob="**/*.docx",
    loader_cls=UnstructuredWordDocumentLoader
)
word_docs = word_loader.load()

# 3. 合并
documents = txt_docs + word_docs
```

### 5.4 文本拆分

* 使用 `RecursiveCharacterTextSplitter`，参数：

  * `chunk_size=1000`
  * `chunk_overlap=200`
* 将文档拆分为多个文本块，并生成 `splits`。
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
)
splits = text_splitter.split_documents(documents)
```
### 5.5 向量存储初始化
* 将嵌入向量和文本块持久化到磁盘，便于重复使用。
```python
from langchain_community.vectorstores import Chroma
vector_store = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="vector_store"
)
```

### 5.6 QA 链构建

* **PromptTemplate**：带有 `{context}` 与 `{question}` 占位符的模板。
* **ChatOpenAI (DeepSeek)**：通过 `api_key` 与 `openai_api_base` 连接 DeepSeek。
* 使用 `RetrievalQA.from_chain_type`：

  * `llm=ChatOpenAI`
  * `chain_type="stuff"`
  * `retriever=vector_store.as_retriever()`
  * `chain_type_kwargs={"prompt": prompt}`

### 5.7 控制台接口

* 在控制台显示 `问题：` 提示。
* 输入 `退出` 或 `exit`（不区分大小写）结束会话。
* 调用 `qa_chain.invoke({"query": question})` 并打印 `result['result']`。

## 6. 数据流程

1. **启动**
   * 加载环境变量与 NLTK 资源。
2. **索引构建**（首次运行或按需重建）
   * 加载文档 → 拆分文档 → 计算嵌入 → 持久化至 Chroma。
3. **查询阶段**
   * 用户输入问题 → 向量存储检索相关文本块 → LLM 生成答案 → 在控制台输出。

## 7. 错误处理与日志

* 捕获 NLTK 下载错误并打印提示。
* 验证 API 响应，处理网络或认证失败。
* 在环境变量缺失时，抛出描述性错误。

## 8. 可扩展性与未来改进

* **文档格式支持**：添加 PDF、HTML 加载器。
* **嵌入模型切换**：支持本地 HuggingFace BGE 离线使用。
* **LLM 选项**：兼容 OpenAI GPT 系列或本地开源模型。
* **Web 界面**：基于 Flask/FastAPI 构建 REST 服务。
* **流式响应**：在控制台实现增量输出。
* **元数据过滤**：按标签、日期等元信息精细检索。

## 9. 安全注意事项

* 安全存储 API 密钥，避免将 `.env` 提交至版本控制系统。
* 对加载文档内容进行清洗，防止注入攻击。
* 对 API 调用进行限流，防止配额耗尽。

## 10. 成果展示
1. 在docs目录中添加需求文档如 `需求说明书_[主项进件系统]-渠道营销码-[微租赁系统需求]-V1.36.docx`
2. 运行 python main.py
3. 控制台出现 `问题：` 并输入自己的问题如 `营销二维码入口在哪？`
4. 应用会根据文档里的内容输出答案，如下图：
 ![成果.png](%E6%88%90%E6%9E%9C.png)
---

*设计文档完*
